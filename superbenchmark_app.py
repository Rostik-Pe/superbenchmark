import os
import json
from datetime import datetime
from typing import List
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel

app = FastAPI(title="SuperBenchmark")


class BenchmarkResult(BaseModel):
    """
        Represents a single benchmarking result.

        Attributes:
            request_id (str): Unique identifier for the benchmarking request.
            prompt_text (str): The input prompt text used for the LLM.
            generated_text (str): The output text generated by the LLM.
            token_count (int): The number of tokens in the generated text.
            time_to_first_token (int): The time taken to generate the first token (in milliseconds).
            time_per_output_token (int): The average time per output token (in milliseconds).
            total_generation_time (int): The total time to generate the response (in milliseconds).
            timestamp (datetime): The timestamp when the benchmarking result was recorded.
        """
    request_id: str
    prompt_text: str
    generated_text: str
    token_count: int
    time_to_first_token: int
    time_per_output_token: int
    total_generation_time: int
    timestamp: datetime


@app.get("/results/average")
def get_average_results() -> dict:
    """
    Returns the average performance statistics across all benchmarking results.
    """
    results = _get_benchmark_results()
    if not results:
        raise HTTPException(status_code=404, detail="No benchmark results found.")

    total_token_count = sum(r.token_count for r in results)
    total_time_to_first_token = sum(r.time_to_first_token for r in results)
    total_time_per_output_token = sum(r.time_per_output_token for r in results)
    total_generation_time = sum(r.total_generation_time for r in results)

    return {
        "average_token_count": total_token_count / len(results),
        "average_time_to_first_token": total_time_to_first_token / len(results),
        "average_time_per_output_token": total_time_per_output_token / len(results),
        "average_total_generation_time": total_generation_time / len(results)
    }


@app.get("/results/average/{start_time}/{end_time}")
def get_average_results_in_time_window(start_time: datetime, end_time: datetime) -> dict:
    """
    Returns the average performance statistics for benchmarking results within a specified time window.
    """
    results = [r for r in _get_benchmark_results() if start_time <= r.timestamp <= end_time]
    if not results:
        raise HTTPException(status_code=404, detail="No benchmark results found within the specified time window.")

    total_token_count = sum(r.token_count for r in results)
    total_time_to_first_token = sum(r.time_to_first_token for r in results)
    total_time_per_output_token = sum(r.time_per_output_token for r in results)
    total_generation_time = sum(r.total_generation_time for r in results)

    return {
        "average_token_count": total_token_count / len(results),
        "average_time_to_first_token": total_time_to_first_token / len(results),
        "average_time_per_output_token": total_time_per_output_token / len(results),
        "average_total_generation_time": total_generation_time / len(results)
    }


def _get_benchmark_results() -> List[BenchmarkResult]:
    """
    Retrieves the benchmark results from the test_database.json file.
    """
    if os.getenv("SUPERBENCHMARK_DEBUG", "True").lower() == "true":
        with open("test_database.json") as f:
            data = json.load(f)
            return [BenchmarkResult(**result) for result in data["benchmarking_results"]]

    raise HTTPException(status_code=501, detail="Feature not ready for live deployment.")


if __name__ == "__main__":

    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=8000)
